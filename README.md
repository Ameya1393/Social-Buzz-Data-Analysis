# Social-Buzz-Data-Analysis
### This Project is part of Accenture Virtual Internship Program
![accenture logo](https://github.com/Ameya1393/Social-Buzz-Data-Analysis/assets/84855509/44a61272-ea98-4a03-8b1b-7432d3db3e5b)

## Client background:
#### Social Buzz was founded by two former engineers from a large social media conglomerate, one from London and the other from San Francisco. They left in 2008 and both met in San Francisco to start their business. They started Social Buzz because they saw an opportunity to build on the foundation that their previous company started by creating a new platform where content took center stage. Social Buzz emphasizes content by keeping all users anonymous, only tracking user reactions on every piece of content. There are over 100 ways that users can react to content, spanning beyond the traditional reactions of likes, dislikes, and comments. This ensures that trending content, as opposed to individual users, is at the forefront of user feeds. 

## Scenario: Our team of dedicated professionals at Accenture has been tasked with analyzing and visualizing data for Social Buzz. This documentation serves as a comprehensive resource to understand the project's objectives, methodologies, and outcomes.

# AIM: - An analysis of their content categories that highlights the top 5 categories with the largest aggregate popularity.

## Tools Used: Microsoft Excel, SQL

## Step 1: Data Modelling

### Data Model - [Data Model Link](https://drive.google.com/drive/folders/17Z_Ypx7xBC9h_RBYq64LiKekYYcjrX4T)

### -> After a thorough review of the data model, it has become evident that merging three key tables—Reaction, Content, and Reaction Types—is essential to categorize the top 5 most popular categories effectively. 

## Step 2: Data Cleaning
### Tasks Performed:
1) **Handling Null Values**: The first task was to identify and remove any Null (missing) values within the Excel sheet. Null values can lead to erroneous analysis results, so it was imperative to eliminate them to maintain data integrity.
2) **Removing Unnecessary Tables**: To streamline our dataset and focus on relevant information, I removed extraneous tables and data that were not pertinent to our analysis objectives. This step helped declutter the dataset and improve overall clarity.
3) **Addressing Formatting Inconsistencies**: I addressed any uneven formatting issues within the data. This included standardizing date formats, text capitalization, and ensuring uniform units of measurement where applicable. Consistent formatting is essential for accurate analysis.
4) **Final Data Validation**: Before proceeding to the analysis phase, I conducted a final comprehensive check on the cleaned data to ensure that it was in an optimal state for analysis. This validation step involved confirming data consistency, accuracy, and completeness.



